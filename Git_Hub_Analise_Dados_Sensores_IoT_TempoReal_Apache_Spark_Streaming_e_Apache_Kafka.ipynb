{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e32795-efee-4635-ab3b-96a69ac3e973",
   "metadata": {},
   "source": [
    "## <font>Análise de Dados de Sensores IoT em Tempo Real com Apache Spark Streaming e Apache Kafka</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e9bda",
   "metadata": {},
   "source": [
    "![title](imagens/kafka.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbafefd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.13\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8cbd72",
   "metadata": {},
   "source": [
    "## Definição do problema e fonte de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ae198",
   "metadata": {},
   "source": [
    "Neste projeto vamos implementar uma solução completa de análise de dados em tempo real usando o Apache Spark e o Apache Kafka. Vamos coletar dados de sensores IoT (Internet of Things) e, em tempo real (à medida que os dados são gerados e coletados), realizar o trabalho de análise e entregar a resposta para um determinado problema de negócio.\n",
    "\n",
    "\n",
    "Este projeto é composto de diversas partes uma vez que toda a infraestrutura necessária será construída passo a passo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd1d80",
   "metadata": {},
   "source": [
    "## Coletando os dados dos sensores IoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e03a4",
   "metadata": {},
   "source": [
    "Para coletar dados dos sensores IoT (Internet of Things), vamos seguir as seguintes etapas:\n",
    "\n",
    "1-\tAbrir o terminal ou prompt de comando e acessar a pasta com o simulador.\n",
    "\n",
    "2-\tExecute o comando abaixo para gerar um arquivo com 10.000 leituras de sensores IoT (pode gerar quantos registros você desejar).\n",
    "\n",
    ">python simulador.py 1000 > ../dados/dados_sensores.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0836e",
   "metadata": {},
   "source": [
    "## Instalação do Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81363733",
   "metadata": {},
   "source": [
    "> Todos os comandos para instalação e execução do Apache Kafka no Windows estão documentados no README deste projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de77becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "#!pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "#!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50a5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3183aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14c6950-525d-4e54-a25e-13e9d6818c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29efd4a",
   "metadata": {},
   "source": [
    "> Precisamos incluir o conector de integração do Spark Streaming com o Apache Kafka. Fique atento à versão do PySpark que está sendo usada.\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5172de34-9787-4f22-b1a7-8e9bb3b7cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "488dcaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "pyspark  : 3.5.0\n",
      "findspark: 2.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efeedac",
   "metadata": {},
   "source": [
    "## Criando a Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35f1973-363c-4112-8440-62fca2410984",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3080\\1913005596.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Cria a sessão Spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mini-Projeto6\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "# Cria a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Mini-Projeto6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8744ef",
   "metadata": {},
   "source": [
    "## Leitura do Kafka Spark Structured Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0de89c-e700-4754-9175-d95777b95a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma subscrição no tópico que tem o streaming de dados que desejamos \"puxar\" os dados.\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"dsamp6\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60991a",
   "metadata": {},
   "source": [
    "## Definição do Schema da Fonte de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c2301-7f4c-48fd-8e5a-cc12384ad834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema dos dados que desejamos capturar para análise (temperatura)\n",
    "esquema_dados_temp = StructType([StructField(\"leitura\", \n",
    "                                             StructType([StructField(\"temperatura\", DoubleType(), True)]), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema global dos dados no streaming\n",
    "esquema_dados = StructType([ \n",
    "    StructField(\"id_sensor\", StringType(), True), \n",
    "    StructField(\"id_equipamento\", StringType(), True), \n",
    "    StructField(\"sensor\", StringType(), True), \n",
    "    StructField(\"data_evento\", StringType(), True), \n",
    "    StructField(\"padrao\", esquema_dados_temp, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf910ee0",
   "metadata": {},
   "source": [
    "## Parse da Fonte de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a505e1-6004-48ab-8275-0cdb96de583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturamos cada linha de dado (cada valor) como string\n",
    "df_conversao = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc58eb-a9b5-4760-8c09-abe6bdd117b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse do formato JSON em dataframe\n",
    "df_conversao = df_conversao.withColumn(\"jsonData\", from_json(col(\"value\"), esquema_dados)).select(\"jsonData.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77250a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conversao.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d91c83",
   "metadata": {},
   "source": [
    "## Preparamos o Dataframe \n",
    "\n",
    "Esse dataframe está no formato que precisamos para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9734cc6-fd90-4f32-aa7f-dafcf5bc5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_conversao_temp_sensor = df_conversao.select(col(\"padrao.leitura.temperatura\").alias(\"temperatura\"), \n",
    "                                               col(\"sensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d725e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conversao_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a99b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não podemos visualizar o dataframe, pois a fonte é de streaming\n",
    "# df_conversao_temp_sensor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4ea01",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da9087-67a4-4f16-8325-7d25dcd8d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui temos o objeto que irá conter nossa análise, o cálculo da média das temperaturas por sensor\n",
    "df_media_temp_sensor = df_conversao_temp_sensor.groupby(\"sensor\").mean(\"temperatura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c515750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_media_temp_sensor = df_media_temp_sensor.select(col(\"sensor\").alias(\"sensor\"), \n",
    "                                                   col(\"avg(temperatura)\").alias(\"media_temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dedc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe8621",
   "metadata": {},
   "source": [
    "Abaixo abrimos o streaming para análise de dados em tempo real, imprimindo o resultado no console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3946d80-165e-47a5-a921-56e3d6e675d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de console\n",
    "query = df_media_temp_sensor.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e8a6d2",
   "metadata": {},
   "source": [
    "Envie novos arquivos para o Kafka a fim de ver a análise em tempo real por aqui. Clique no botão Stop no menu superior para interromper a célula a qualquer momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executamos a query do streaming e evitamos que o processo seja encerrado\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41028bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e469f93",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65867a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de memória (cria tabela temporária)\n",
    "query_memoria = df_media_temp_sensor \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"dsa\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706168fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streams ativados\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos manter a query executando por algum tempo e aplicando SQL aos dados em tempo real\n",
    "from time import sleep\n",
    "\n",
    "for x in range(10):\n",
    "    \n",
    "    spark.sql(\"select sensor, round(media_temp, 2) as media from dsa where media_temp > 65\").show()\n",
    "    sleep(3)\n",
    "    \n",
    "query_memoria.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36441eef",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
